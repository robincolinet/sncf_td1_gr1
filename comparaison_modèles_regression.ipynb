{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.multioutput import RegressorChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_data, creer_feature_ligne, creer_features_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "creer_features_date(data)\n",
    "creer_feature_ligne(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost pour prédire seulement le retard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    # Éviter de diviser par zéro\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 60 is smaller than n_iter=100. Running 60 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 300 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n260 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\", line 171, in fit\n    sample_weight, estimator_weight, estimator_error = self._boost(\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\", line 1168, in _boost\n    estimator.fit(X_, y_)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1320, in fit\n    super()._fit(\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 242, in _fit\n    X, y = self._validate_data(\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 617, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'National'\n\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\", line 171, in fit\n    sample_weight, estimator_weight, estimator_error = self._boost(\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\", line 1168, in _boost\n    estimator.fit(X_, y_)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1320, in fit\n    super()._fit(\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 242, in _fit\n    X, y = self._validate_data(\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 617, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'International'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Utilisation de RandomizedSearchCV pour optimiser les hyperparamètres\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ada_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(ada, param_distributions\u001b[38;5;241m=\u001b[39mparam_grid_ada, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mada_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Meilleurs hyperparamètres pour AdaBoost\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(ada_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1809\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1808\u001b[0m     \u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1809\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1810\u001b[0m         ParameterSampler(\n\u001b[1;32m   1811\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[1;32m   1812\u001b[0m         )\n\u001b[1;32m   1813\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[1;32m    869\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    870\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[1;32m    873\u001b[0m     )\n\u001b[0;32m--> 875\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[1;32m    877\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[1;32m    408\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    409\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m     )\n\u001b[0;32m--> 414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    416\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    418\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 300 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n260 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\", line 171, in fit\n    sample_weight, estimator_weight, estimator_error = self._boost(\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\", line 1168, in _boost\n    estimator.fit(X_, y_)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1320, in fit\n    super()._fit(\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 242, in _fit\n    X, y = self._validate_data(\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 617, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'National'\n\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\", line 171, in fit\n    sample_weight, estimator_weight, estimator_error = self._boost(\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\", line 1168, in _boost\n    estimator.fit(X_, y_)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1320, in fit\n    super()._fit(\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 242, in _fit\n    X, y = self._validate_data(\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 617, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/Users/yanivbenchetrit/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'International'\n"
     ]
    }
   ],
   "source": [
    "X = data[['service', 'gare_depart', 'gare_arrivee', 'duree_moyenne', 'nb_train_prevu', 'ligne', 'mois']]\n",
    "y = data['retard_moyen_arrivee']\n",
    "\n",
    "X = pd.get_dummies(X, columns=['gare_depart', 'gare_arrivee', 'service', 'ligne'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialisation de l'estimateur AdaBoost\n",
    "ada = AdaBoostRegressor()\n",
    "\n",
    "# Grille d'hyperparamètres pour AdaBoost\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "    'loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "# Utilisation de RandomizedSearchCV pour optimiser les hyperparamètres\n",
    "ada_search = RandomizedSearchCV(ada, param_distributions=param_grid_ada, n_iter=100, cv=5, verbose=0, n_jobs=-1, random_state=42)\n",
    "ada_search.fit(X_train, y_train)\n",
    "\n",
    "# Meilleurs hyperparamètres pour AdaBoost\n",
    "print(ada_search.best_params_)\n",
    "\n",
    "# Prédiction avec le modèle AdaBoost optimisé\n",
    "y_pred_ada = ada_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Métriques d'évaluation pour AdaBoost\n",
    "mae_ada = mean_absolute_error(y_test, y_pred_ada)\n",
    "mse_ada = mean_squared_error(y_test, y_pred_ada)\n",
    "mape_ada = mean_absolute_percentage_error(y_test, y_pred_ada)\n",
    "r2_ada = r2_score(y_test, y_pred_ada)\n",
    "\n",
    "print(f\"Mean Squared Error with AdaBoost: {mse_ada}\")\n",
    "print(f\"Mean Absolute Error with AdaBoost: {mae_ada}\")\n",
    "print(f\"Mean Absolute Percentage Error with AdaBoost: {mape_ada}\")\n",
    "print(f\"R2 Score with AdaBoost: {r2_ada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest avec hyperparamètres optimisés par Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cibles = ['retard_moyen_arrivee','prct_cause_externe', 'prct_cause_infra', 'prct_cause_gestion_trafic',\n",
    "       'prct_cause_materiel_roulant', 'prct_cause_gestion_gare',\n",
    "       'prct_cause_prise_en_charge_voyageurs']\n",
    "\n",
    "X = data[['service', 'gare_depart', 'gare_arrivee', 'duree_moyenne', 'nb_train_prevu', 'ligne', 'mois']]\n",
    "y = data[cibles]\n",
    "\n",
    "# One-Hot Encoding pour les colonnes catégorielles\n",
    "X = pd.get_dummies(X, columns=['gare_depart', 'gare_arrivee', 'service', 'ligne'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\apprauto\\sncf_td1_gr1\\comparaison_modèles_ML.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/comparaison_mod%C3%A8les_ML.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Recherche aléatoire avec validation croisée\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/comparaison_mod%C3%A8les_ML.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m rf_search \u001b[39m=\u001b[39m RandomizedSearchCV(rf, param_distributions\u001b[39m=\u001b[39mparam_grid_randomized, n_iter\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/comparaison_mod%C3%A8les_ML.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m rf_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/comparaison_mod%C3%A8les_ML.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Meilleurs hyperparamètres\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/comparaison_mod%C3%A8les_ML.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMeilleurs paramètres : \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,rf_search\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1753\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1751\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1752\u001b[0m     \u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1753\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1754\u001b[0m         ParameterSampler(\n\u001b[0;32m   1755\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1756\u001b[0m         )\n\u001b[0;32m   1757\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m         clone(base_estimator),\n\u001b[0;32m    825\u001b[0m         X,\n\u001b[0;32m    826\u001b[0m         y,\n\u001b[0;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    836\u001b[0m     )\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1939\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1944\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1584\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1587\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1590\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1698\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1699\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1700\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid_randomized = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 5, 10, 20, 50, 75],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "\n",
    "# Recherche aléatoire avec validation croisée\n",
    "rf_search = RandomizedSearchCV(rf, param_distributions=param_grid_randomized, n_iter=100, cv=5, verbose=0, n_jobs=-1, random_state=42)\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "# Meilleurs hyperparamètres\n",
    "print(\"Meilleurs paramètres : \\n\",rf_search.best_params_)\n",
    "\n",
    "#y_pred_grid = grid_search.best_estimator_.predict(X_test)\n",
    "y_pred_randomized = rf_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Séparation des prédictions de retard_moyen_arrivee\n",
    "y_pred_retard = y_pred_randomized[:, 0]\n",
    "\n",
    "# Prédictions pour les autres colonnes\n",
    "y_pred_cause = y_pred_randomized[:, 1:]\n",
    "\n",
    "# Normalisation de ces prédictions pour qu'elles somment à 100\n",
    "sums = y_pred_cause.sum(axis=1)[:, np.newaxis]\n",
    "y_pred_normalized = 100 * y_pred_cause / sums\n",
    "\n",
    "# Ajout des prédictions de retard_moyen_arrivee aux prédictions normalisées\n",
    "y_pred_final = np.hstack([y_pred_retard[:, np.newaxis], y_pred_normalized])\n",
    "\n",
    "for i, cible in enumerate(cibles):\n",
    "    mae = mean_absolute_error(y_test[cible], y_pred_final[:, i])\n",
    "    mse_randomized = mean_squared_error(y_test[cible], y_pred_final[:, i])\n",
    "    mape = mean_absolute_percentage_error(y_test[cible], y_pred_final[:, i])\n",
    "    r2 = r2_score(y_test[cible], y_pred_final[:, i])\n",
    "    \n",
    "    print(f\"--- {cible} ---\")\n",
    "    print(f\"Mean Squared Error with RandomizedSearch: {mse_randomized}\")\n",
    "    print(f\"Mean Absolute Error with RandomizedSearch: {mae}\")\n",
    "    print(f\"Mean Absolute Percentage Error with RandomizedSearch: {mape}\")\n",
    "    print(f\"R2 Score with RandomizedSearch: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\apprauto\\sncf_td1_gr1\\comparaison_modèles_ML.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/comparaison_mod%C3%A8les_ML.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m best_rf \u001b[39m=\u001b[39m rf_search\u001b[39m.\u001b[39;49mbest_estimator_ \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/comparaison_mod%C3%A8les_ML.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Utilisation du modèle optimisé dans RegressorChain\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/comparaison_mod%C3%A8les_ML.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m chain_rf \u001b[39m=\u001b[39m RegressorChain(best_rf)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "best_rf = rf_search.best_estimator_ \n",
    "\n",
    "# Utilisation du modèle optimisé dans RegressorChain\n",
    "chain_rf = RegressorChain(best_rf)\n",
    "chain_rf.fit(X_train, y_train)\n",
    "y_pred_chain = chain_rf.predict(X_test)\n",
    "\n",
    "# Séparation des prédictions de retard_moyen_arrivee\n",
    "y_pred_retard = y_pred_chain[:, 0]\n",
    "\n",
    "# Prédictions pour les autres colonnes\n",
    "y_pred_cause = y_pred_chain[:, 1:]\n",
    "\n",
    "# Normalisation de ces prédictions pour qu'elles somment à 100\n",
    "sums = y_pred_cause.sum(axis=1)[:, np.newaxis]\n",
    "y_pred_normalized = 100 * y_pred_cause / sums\n",
    "\n",
    "# Ajout des prédictions de retard_moyen_arrivee aux prédictions normalisées\n",
    "y_pred_final = np.hstack([y_pred_retard[:, np.newaxis], y_pred_normalized])\n",
    "\n",
    "for i, cible in enumerate(cibles):\n",
    "    mae = mean_absolute_error(y_test[cible], y_pred_final[:, i])\n",
    "    mse_randomized = mean_squared_error(y_test[cible], y_pred_final[:, i])\n",
    "    mape = mean_absolute_percentage_error(y_test[cible], y_pred_final[:, i])\n",
    "    r2 = r2_score(y_test[cible], y_pred_final[:, i])\n",
    "    \n",
    "    print(f\"--- {cible} ---\")\n",
    "    print(f\"Mean Squared Error with RandomizedSearch: {mse_randomized}\")\n",
    "    print(f\"Mean Absolute Error with RandomizedSearch: {mae}\")\n",
    "    print(f\"Mean Absolute Percentage Error with RandomizedSearch: {mape}\")\n",
    "    print(f\"R2 Score with RandomizedSearch: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5647cfce6fa2861c5d5993eed494c4303d5a706b1125d41555b49a965ed01245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
