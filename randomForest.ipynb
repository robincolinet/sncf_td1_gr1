{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_data, creer_feature_ligne, preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.multioutput import RegressorChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    # Éviter de diviser par zéro\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "preprocess_data(data)\n",
    "creer_feature_ligne(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plusieurs clusterings + RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cibles = ['retard_moyen_arrivee','prct_cause_externe', 'prct_cause_infra', 'prct_cause_gestion_trafic',\n",
    "       'prct_cause_materiel_roulant', 'prct_cause_gestion_gare',\n",
    "       'prct_cause_prise_en_charge_voyageurs']\n",
    "\n",
    "X = data[['service', 'gare_depart', 'gare_arrivee', 'duree_moyenne', 'nb_train_prevu', 'ligne', 'mois']]\n",
    "y = data[cibles]\n",
    "\n",
    "# One-Hot Encoding pour les colonnes catégorielles\n",
    "X = pd.get_dummies(X, columns=['gare_depart', 'gare_arrivee', 'service', 'ligne'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\apprauto\\sncf_td1_gr1\\randomForest.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/randomForest.ipynb#X15sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Recherche aléatoire avec validation croisée\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/randomForest.ipynb#X15sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m rf_search \u001b[39m=\u001b[39m RandomizedSearchCV(rf, param_distributions\u001b[39m=\u001b[39mparam_grid, n_iter\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/randomForest.ipynb#X15sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m rf_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/randomForest.ipynb#X15sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# Meilleurs hyperparamètres\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/apprauto/sncf_td1_gr1/randomForest.ipynb#X15sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMeilleurs paramètres : \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,rf_search\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1753\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1751\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1752\u001b[0m     \u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1753\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1754\u001b[0m         ParameterSampler(\n\u001b[0;32m   1755\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1756\u001b[0m         )\n\u001b[0;32m   1757\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m         clone(base_estimator),\n\u001b[0;32m    825\u001b[0m         X,\n\u001b[0;32m    826\u001b[0m         y,\n\u001b[0;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    836\u001b[0m     )\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1939\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1944\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1584\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1587\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1590\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1698\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1699\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1700\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Ajout d'une feature en utilisant un clustering\n",
    "\n",
    "# One-Hot Encoding pour les colonnes catégorielles\n",
    "#X_cluster = pd.get_dummies(data, columns=['date','gare_depart', 'gare_arrivee', 'service', 'ligne'])\n",
    "#X_cluster = X_cluster.drop(['commentaire_annulation', 'commentaire_retards_depart', 'commentaires_retard_arrivee'], axis=1)\n",
    "\n",
    "#Trouvé à l'aide de l'elbow méthode\n",
    "k_optimal = 3\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42)\n",
    "#data['cluster'] = kmeans.fit_predict(X_train)\n",
    "\n",
    "# CLUSTERING \n",
    "#majority_class = {}\n",
    "#lignes= data['ligne'].unique()\n",
    "#for ligne in lignes:\n",
    "#    cluster_labels = data[data['ligne'] == ligne]['cluster'].tolist()\n",
    "#    majority_label = Counter(cluster_labels).most_common(1)[0][0]\n",
    "#    majority_class[ligne] = majority_label\n",
    "\n",
    "\n",
    "#data['majority_class'] = data['ligne'].map(majority_class)\n",
    "\n",
    "X_train['cluster'] = kmeans.fit_predict(X_train)\n",
    "X_test['cluster'] = kmeans.predict(X_test)\n",
    "\n",
    "\n",
    "param_grid_randomized = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 5, 10, 20, 50, 75],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "param_grid_multi_rf = {\n",
    "    'estimator__n_estimators': [10, 50, 100, 200],\n",
    "    'estimator__max_depth': [None, 10, 20, 30]\n",
    "}\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "multi_rf = MultiOutputRegressor(rf)\n",
    "\n",
    "\n",
    "# Recherche aléatoire avec validation croisée\n",
    "rf_search = RandomizedSearchCV(rf, param_distributions=param_grid_randomized, n_iter=100, cv=5, verbose=0, n_jobs=-1, random_state=42)\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "# Meilleurs hyperparamètres\n",
    "print(\"Meilleurs paramètres : \\n\",rf_search.best_params_)\n",
    "\n",
    "#y_pred_grid = grid_search.best_estimator_.predict(X_test)\n",
    "y_pred_randomized = rf_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Séparation des prédictions de retard_moyen_arrivee\n",
    "y_pred_retard = y_pred_randomized[:, 0]\n",
    "\n",
    "# Prédictions pour les autres colonnes\n",
    "y_pred_cause = y_pred_randomized[:, 1:]\n",
    "\n",
    "# Normalisation de ces prédictions pour qu'elles somment à 100\n",
    "sums = y_pred_cause.sum(axis=1)[:, np.newaxis]\n",
    "y_pred_normalized = 100 * y_pred_cause / sums\n",
    "\n",
    "# Ajout des prédictions de retard_moyen_arrivee aux prédictions normalisées\n",
    "y_pred_final = np.hstack([y_pred_retard[:, np.newaxis], y_pred_normalized])\n",
    "\n",
    "for i, cible in enumerate(cibles):\n",
    "    mae = mean_absolute_error(y_test[cible], y_pred_final[:, i])\n",
    "    mse_randomized = mean_squared_error(y_test[cible], y_pred_final[:, i])\n",
    "    mape = mean_absolute_percentage_error(y_test[cible], y_pred_final[:, i])\n",
    "    r2 = r2_score(y_test[cible], y_pred_final[:, i])\n",
    "    \n",
    "    print(f\"--- {cible} ---\")\n",
    "    print(f\"Mean Squared Error with RandomizedSearch: {mse_randomized}\")\n",
    "    print(f\"Mean Absolute Error with RandomizedSearch: {mae}\")\n",
    "    print(f\"Mean Absolute Percentage Error with RandomizedSearch: {mape}\")\n",
    "    print(f\"R2 Score with RandomizedSearch: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF optimisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error with RandomizedSearch: 8.6689234939845\n",
      "R2 Score with RandomizedSearch: 0.2804701547444919\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=50, min_samples_split=2, min_samples_leaf=4, max_features='sqrt', max_depth=None, bootstrap=True)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error with RandomizedSearch: {mae}\")\n",
    "print(f\"R2 Score with RandomizedSearch: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Différence ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres : \n",
      " {'n_estimators': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None, 'bootstrap': True}\n",
      "--- retard_moyen_arrivee ---\n",
      "Mean Squared Error with RandomizedSearch: 197.549625429174\n",
      "Mean Absolute Error with RandomizedSearch: 8.78776625571921\n",
      "Mean Absolute Percentage Error with RandomizedSearch: 25.90764092955535\n",
      "R2 Score with RandomizedSearch: 0.2697557892588055\n",
      "--- prct_cause_externe ---\n",
      "Mean Squared Error with RandomizedSearch: 274.7945596989414\n",
      "Mean Absolute Error with RandomizedSearch: 12.327194885178798\n",
      "Mean Absolute Percentage Error with RandomizedSearch: 60.49619538289171\n",
      "R2 Score with RandomizedSearch: 0.04381874740491387\n",
      "--- prct_cause_infra ---\n",
      "Mean Squared Error with RandomizedSearch: 225.73687964727796\n",
      "Mean Absolute Error with RandomizedSearch: 11.138299400832386\n",
      "Mean Absolute Percentage Error with RandomizedSearch: 55.42120707483746\n",
      "R2 Score with RandomizedSearch: 0.12054349095025085\n",
      "--- prct_cause_gestion_trafic ---\n",
      "Mean Squared Error with RandomizedSearch: 196.68311023834468\n",
      "Mean Absolute Error with RandomizedSearch: 10.23217535328513\n",
      "Mean Absolute Percentage Error with RandomizedSearch: 55.761047906129726\n",
      "R2 Score with RandomizedSearch: 0.12729579044874195\n",
      "--- prct_cause_materiel_roulant ---\n",
      "Mean Squared Error with RandomizedSearch: 162.57181960000787\n",
      "Mean Absolute Error with RandomizedSearch: 9.147194177772302\n",
      "Mean Absolute Percentage Error with RandomizedSearch: 49.38646544863546\n",
      "R2 Score with RandomizedSearch: 0.18050521813169895\n",
      "--- prct_cause_gestion_gare ---\n",
      "Mean Squared Error with RandomizedSearch: 60.357861996275936\n",
      "Mean Absolute Error with RandomizedSearch: 5.2933786060796715\n",
      "Mean Absolute Percentage Error with RandomizedSearch: 57.84784873650637\n",
      "R2 Score with RandomizedSearch: 0.06556631383737732\n",
      "--- prct_cause_prise_en_charge_voyageurs ---\n",
      "Mean Squared Error with RandomizedSearch: 93.64172937042865\n",
      "Mean Absolute Error with RandomizedSearch: 6.193824945826369\n",
      "Mean Absolute Percentage Error with RandomizedSearch: 72.66281439841504\n",
      "R2 Score with RandomizedSearch: 0.14642844254450949\n"
     ]
    }
   ],
   "source": [
    "k_optimal = 3\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42)\n",
    "\n",
    "X_train['cluster'], X_test['cluster'] = kmeans.fit_predict(X_train), kmeans.predict(X_test)\n",
    "rf = RandomForestRegressor()\n",
    "# multi_rf = MultiOutputRegressor(rf)\n",
    "\n",
    "# Recherche aléatoire avec validation croisée\n",
    "rf_search = RandomizedSearchCV(rf, param_distributions=param_grid_randomized, n_iter=1, cv=5, verbose=0, n_jobs=-1, random_state=42)\n",
    "rf_search.fit(X_train, y_train)\n",
    "print(\"Meilleurs paramètres : \\n\",rf_search.best_params_)\n",
    "\n",
    "y_pred_randomized = rf_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Séparation des prédictions de retard_moyen_arrivee\n",
    "y_pred_retard = y_pred_randomized[:, 0]\n",
    "\n",
    "# Prédictions pour les autres colonnes\n",
    "y_pred_cause = y_pred_randomized[:, 1:]\n",
    "\n",
    "# Normalisation de ces prédictions pour qu'elles somment à 100\n",
    "sums = y_pred_cause.sum(axis=1)[:, np.newaxis]\n",
    "y_pred_normalized = 100 * y_pred_cause / sums\n",
    "\n",
    "# Ajout des prédictions de retard_moyen_arrivee aux prédictions normalisées\n",
    "y_pred_final = np.hstack([y_pred_retard[:, np.newaxis], y_pred_normalized])\n",
    "\n",
    "for i, cible in enumerate(cibles):\n",
    "    mae = mean_absolute_error(y_test[cible], y_pred_final[:, i])\n",
    "    mse_randomized = mean_squared_error(y_test[cible], y_pred_final[:, i])\n",
    "    mape = mean_absolute_percentage_error(y_test[cible], y_pred_final[:, i])\n",
    "    r2 = r2_score(y_test[cible], y_pred_final[:, i])\n",
    "    \n",
    "    print(f\"--- {cible} ---\")\n",
    "    print(f\"Mean Squared Error with RandomizedSearch: {mse_randomized}\")\n",
    "    print(f\"Mean Absolute Error with RandomizedSearch: {mae}\")\n",
    "    print(f\"Mean Absolute Percentage Error with RandomizedSearch: {mape}\")\n",
    "    print(f\"R2 Score with RandomizedSearch: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- retard_moyen_arrivee ---\n",
      "Mean Squared Error with RegressorChain: 197.5415713502332\n",
      "Mean Absolute Error with RegressorChain: 8.668389372662315\n",
      "Mean Absolute Percentage Error with RegressorChain: 25.596084513701605\n",
      "R2 Score with RegressorChain: 0.26978556124397923\n",
      "--- prct_cause_externe ---\n",
      "Mean Squared Error with RegressorChain: 245.6393074708426\n",
      "Mean Absolute Error with RegressorChain: 11.732289238593191\n",
      "Mean Absolute Percentage Error with RegressorChain: 60.17115549653846\n",
      "R2 Score with RegressorChain: 0.14526801054073202\n",
      "--- prct_cause_infra ---\n",
      "Mean Squared Error with RegressorChain: 216.94093418809442\n",
      "Mean Absolute Error with RegressorChain: 10.879777551502835\n",
      "Mean Absolute Percentage Error with RegressorChain: 55.010910665651465\n",
      "R2 Score with RegressorChain: 0.1548119343672627\n",
      "--- prct_cause_gestion_trafic ---\n",
      "Mean Squared Error with RegressorChain: 182.444769503909\n",
      "Mean Absolute Error with RegressorChain: 10.078631272222765\n",
      "Mean Absolute Percentage Error with RegressorChain: 57.39324233355242\n",
      "R2 Score with RegressorChain: 0.19047284658187547\n",
      "--- prct_cause_materiel_roulant ---\n",
      "Mean Squared Error with RegressorChain: 156.17452340861965\n",
      "Mean Absolute Error with RegressorChain: 9.040228552992057\n",
      "Mean Absolute Percentage Error with RegressorChain: 50.447650197473806\n",
      "R2 Score with RegressorChain: 0.21275281713014393\n",
      "--- prct_cause_gestion_gare ---\n",
      "Mean Squared Error with RegressorChain: 59.2165822819143\n",
      "Mean Absolute Error with RegressorChain: 5.333469148251857\n",
      "Mean Absolute Percentage Error with RegressorChain: 56.37403053503783\n",
      "R2 Score with RegressorChain: 0.08323510088784258\n",
      "--- prct_cause_prise_en_charge_voyageurs ---\n",
      "Mean Squared Error with RegressorChain: 97.84976851606447\n",
      "Mean Absolute Error with RegressorChain: 6.239360484883746\n",
      "Mean Absolute Percentage Error with RegressorChain: 68.44942541163937\n",
      "R2 Score with RegressorChain: 0.1080709436866516\n"
     ]
    }
   ],
   "source": [
    "best_rf = rf_search.best_estimator_ \n",
    "\n",
    "# Utilisation du modèle optimisé dans RegressorChain\n",
    "chain_rf = RegressorChain(best_rf)\n",
    "chain_rf.fit(X_train, y_train)\n",
    "y_pred_chain = chain_rf.predict(X_test)\n",
    "\n",
    "# Séparation des prédictions de retard_moyen_arrivee\n",
    "y_pred_retard = y_pred_chain[:, 0]\n",
    "\n",
    "# Prédictions pour les autres colonnes\n",
    "y_pred_cause = y_pred_chain[:, 1:]\n",
    "\n",
    "# Normalisation de ces prédictions pour qu'elles somment à 100\n",
    "sums = y_pred_cause.sum(axis=1)[:, np.newaxis]\n",
    "y_pred_normalized = 100 * y_pred_cause / sums\n",
    "\n",
    "# Ajout des prédictions de retard_moyen_arrivee aux prédictions normalisées\n",
    "y_pred_final = np.hstack([y_pred_retard[:, np.newaxis], y_pred_normalized])\n",
    "\n",
    "for i, cible in enumerate(cibles):\n",
    "    mae = mean_absolute_error(y_test[cible], y_pred_final[:, i])\n",
    "    mse_randomized = mean_squared_error(y_test[cible], y_pred_final[:, i])\n",
    "    mape = mean_absolute_percentage_error(y_test[cible], y_pred_final[:, i])\n",
    "    r2 = r2_score(y_test[cible], y_pred_final[:, i])\n",
    "    \n",
    "    print(f\"--- {cible} ---\")\n",
    "    print(f\"Mean Squared Error with RandomizedSearch: {mse_randomized}\")\n",
    "    print(f\"Mean Absolute Error with RandomizedSearch: {mae}\")\n",
    "    print(f\"Mean Absolute Percentage Error with RandomizedSearch: {mape}\")\n",
    "    print(f\"R2 Score with RandomizedSearch: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Regressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "# Initialisation de l'estimateur AdaBoost\n",
    "ada = AdaBoostRegressor()\n",
    "\n",
    "# Grille d'hyperparamètres pour AdaBoost\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "    'loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "# Utilisation de RandomizedSearchCV pour optimiser les hyperparamètres\n",
    "ada_search = RandomizedSearchCV(ada, param_distributions=param_grid_ada, n_iter=100, cv=5, verbose=0, n_jobs=-1, random_state=42)\n",
    "ada_search.fit(X_train, y_train)\n",
    "\n",
    "# Meilleurs hyperparamètres pour AdaBoost\n",
    "print(ada_search.best_params_)\n",
    "\n",
    "# Prédiction avec le modèle AdaBoost optimisé\n",
    "y_pred_ada = ada_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Métriques d'évaluation pour AdaBoost\n",
    "mae_ada = mean_absolute_error(y_test, y_pred_ada)\n",
    "mse_ada = mean_squared_error(y_test, y_pred_ada)\n",
    "mape_ada = mean_absolute_percentage_error(y_test, y_pred_ada)\n",
    "r2_ada = r2_score(y_test, y_pred_ada)\n",
    "\n",
    "print(f\"Mean Squared Error with AdaBoost: {mse_ada}\")\n",
    "print(f\"Mean Absolute Error with AdaBoost: {mae_ada}\")\n",
    "print(f\"Mean Absolute Percentage Error with AdaBoost: {mape_ada}\")\n",
    "print(f\"R2 Score with AdaBoost: {r2_ada}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       ligne  cluster\n",
      "0        BORDEAUX ST JEAN-PARIS MONTPARNASSE        0\n",
      "1       LA ROCHELLE VILLE-PARIS MONTPARNASSE        2\n",
      "2                 PARIS MONTPARNASSE-QUIMPER        2\n",
      "3                 PARIS MONTPARNASSE-ST MALO        1\n",
      "4     PARIS MONTPARNASSE-ST PIERRE DES CORPS        0\n",
      "...                                      ...      ...\n",
      "8149                    STRASBOURG-PARIS EST        0\n",
      "8150    TOULOUSE MATABIAU-PARIS MONTPARNASSE        1\n",
      "8151                TOURS-PARIS MONTPARNASSE        2\n",
      "8152           VALENCE ALIXAN TGV-PARIS LYON        0\n",
      "8153               VANNES-PARIS MONTPARNASSE        2\n",
      "\n",
      "[8154 rows x 2 columns]\n",
      "0       0\n",
      "1       2\n",
      "2       2\n",
      "3       1\n",
      "4       0\n",
      "       ..\n",
      "8149    0\n",
      "8150    1\n",
      "8151    2\n",
      "8152    2\n",
      "8153    2\n",
      "Name: majority_class, Length: 8154, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "k_optimal = 3\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42)\n",
    "data['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "print(data[['ligne', 'cluster']])\n",
    "\n",
    "majority_class = {}\n",
    "lignes= data['ligne'].unique()\n",
    "for ligne in lignes:\n",
    "    cluster_labels = data[data['ligne'] == ligne]['cluster'].tolist()\n",
    "    majority_label = Counter(cluster_labels).most_common(1)[0][0]\n",
    "    majority_class[ligne] = majority_label\n",
    "\n",
    "\n",
    "data['majority_class'] = data['ligne'].map(majority_class)\n",
    "\n",
    "X['majority_class'] = data['majority_class']\n",
    "\n",
    "print(X['majority_class'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5647cfce6fa2861c5d5993eed494c4303d5a706b1125d41555b49a965ed01245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
